{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed954ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq ë‹¤ë¤„ë³´ê¸°\n"
     ]
    }
   ],
   "source": [
    "print(\"Groq ë‹¤ë¤„ë³´ê¸°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2429280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_y\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc9b534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['prompt'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['prompt'], input_types={}, partial_variables={}, template='{prompt}'), additional_kwargs={})]\n",
      "System: ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.\n",
      "Human: AI ê°œë°œìë€ ë¬´ì—‡ì¼ê¹Œìš”?\n"
     ]
    }
   ],
   "source": [
    "# Prompt + LLM + output\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.\"),\n",
    "    (\"user\", \"{prompt}\")\n",
    "])\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(prompt=\"AI ê°œë°œìë€ ë¬´ì—‡ì¼ê¹Œìš”?\")\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b880c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‘ë‹µ:  AI ê°œë°œìë€ **ì¸ê³µì§€ëŠ¥(AI) ê¸°ìˆ ì„ ì„¤ê³„Â·êµ¬í˜„Â·ìš´ì˜í•˜ëŠ” ì „ë¬¸ê°€**ë¥¼ ë§í•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìê°€ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œì§, ë°ì´í„°ë² ì´ìŠ¤, í”„ë¡ íŠ¸ì—”ë“œÂ·ë°±ì—”ë“œ ë“±ì„ ë‹¤ë£¨ëŠ” ë° ë¹„í•´, AI ê°œë°œìëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "\n",
      "| ë¶„ì•¼ | ì£¼ìš” ì—…ë¬´ | ì‚¬ìš© ê¸°ìˆ Â·íˆ´ |\n",
      "|------|-----------|--------------|\n",
      "| **ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§** | í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ìˆ˜ì§‘Â·ì •ì œÂ·ì „ì²˜ë¦¬ | Python (pandas, NumPy), SQL, Apache Spark, Airflow |\n",
      "| **ëª¨ë¸ ì„¤ê³„Â·í•™ìŠµ** | ë¬¸ì œì— ë§ëŠ” ì•Œê³ ë¦¬ì¦˜ ì„ íƒÂ·ëª¨ë¸ êµ¬ì¡° ì„¤ê³„Â·í•™ìŠµ íŒŒë¼ë¯¸í„° íŠœë‹ | TensorFlow, PyTorch, Keras, scikitâ€‘learn, JAX |\n",
      "| **ëª¨ë¸ í‰ê°€Â·ê²€ì¦** | ì •í™•ë„, ì •ë°€ë„Â·ì¬í˜„ìœ¨, ROCâ€‘AUC ë“± ì§€í‘œ ê³„ì‚°Â·êµì°¨ ê²€ì¦Â·ì˜¤ë²„í”¼íŒ… ë°©ì§€ | MLflow, Weights & Biases, TensorBoard |\n",
      "| **ë°°í¬Â·ì„œë¹™** | ëª¨ë¸ì„ APIÂ·ì•±Â·ì—£ì§€ ë””ë°”ì´ìŠ¤ì— ë°°í¬Â·ì‹¤ì‹œê°„ ì¶”ë¡ Â·ìŠ¤ì¼€ì¼ë§ | Docker, Kubernetes, FastAPI, TensorRT, ONNX, AWS SageMaker, Azure ML |\n",
      "| **ëª¨ë‹ˆí„°ë§Â·ìš´ì˜** | ì¶”ë¡  ì„±ëŠ¥, ë°ì´í„° ë“œë¦¬í”„íŠ¸, ì˜¤ë¥˜ ë¡œê·¸ ê°ì‹œÂ·ì£¼ê¸°ì  ì¬í•™ìŠµ | Prometheus, Grafana, ELK stack, Evidently AI |\n",
      "| **ìœ¤ë¦¬Â·ë²•ê·œ** | í¸í–¥Â·ê³µì •ì„±Â·í”„ë¼ì´ë²„ì‹œ ê²€í† Â·ê·œì œ ì¤€ìˆ˜ | IBM AI Fairness 360, Google Whatâ€‘If Tool, GDPR/CCPA ê°€ì´ë“œë¼ì¸ |\n",
      "\n",
      "### AI ê°œë°œìê°€ ê°–ì¶°ì•¼ í•  í•µì‹¬ ì—­ëŸ‰\n",
      "\n",
      "1. **í”„ë¡œê·¸ë˜ë° ì‹¤ë ¥**  \n",
      "   - Pythonì´ ê¸°ë³¸ì´ë©°, C++/Java ë“± ì„±ëŠ¥ ìµœì í™”ê°€ í•„ìš”í•œ ê²½ìš°ë„ ìˆìŒ.  \n",
      "2. **ìˆ˜í•™Â·í†µê³„ ì§€ì‹**  \n",
      "   - ì„ í˜•ëŒ€ìˆ˜, í™•ë¥ Â·í†µê³„, ìµœì í™” ì´ë¡ ì„ ì´í•´í•´ì•¼ ëª¨ë¸ì„ ì„¤ê³„í•˜ê³  íŠœë‹í•  ìˆ˜ ìˆë‹¤.  \n",
      "3. **ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ì´ë¡ **  \n",
      "   - ì§€ë„í•™ìŠµ, ë¹„ì§€ë„í•™ìŠµ, ê°•í™”í•™ìŠµ, íŠ¸ëœìŠ¤í¬ë¨¸, CNN, RNN ë“± ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì´í•´.  \n",
      "4. **ë°ì´í„° ì²˜ë¦¬ ëŠ¥ë ¥**  \n",
      "   - ëŒ€ê·œëª¨ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•Â·ETL, ê²°ì¸¡ì¹˜Â·ë…¸ì´ì¦ˆ ì²˜ë¦¬ ê²½í—˜.  \n",
      "5. **í´ë¼ìš°ë“œÂ·DevOps ì§€ì‹**  \n",
      "   - AWS, GCP, Azure ë“± í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ì™€ CI/CD, ì»¨í…Œì´ë„ˆí™”, ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ê²½í—˜.  \n",
      "6. **ë¬¸ì œ í•´ê²°Â·ë¹„ì¦ˆë‹ˆìŠ¤ ì´í•´**  \n",
      "   - ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œë¥¼ AI ì†”ë£¨ì…˜ìœ¼ë¡œ ì „í™˜í•˜ê³ , ROIë¥¼ í‰ê°€í•  ìˆ˜ ìˆì–´ì•¼ í•¨.  \n",
      "7. **ìœ¤ë¦¬Â·ì±…ì„ê°**  \n",
      "   - AI ëª¨ë¸ì´ ì´ˆë˜í•  ìˆ˜ ìˆëŠ” ì‚¬íšŒì Â·ë²•ì  ì˜í–¥ì„ ê³ ë ¤í•˜ê³ , íˆ¬ëª…ì„±ì„ í™•ë³´í•œë‹¤.  \n",
      "\n",
      "### AI ê°œë°œì ìœ í˜• (ì˜ˆì‹œ)\n",
      "\n",
      "| ìœ í˜• | ì´ˆì  | ëŒ€í‘œ ì—…ë¬´ |\n",
      "|------|------|-----------|\n",
      "| **ì—°êµ¬í˜•** | ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜Â·ë…¼ë¬¸ êµ¬í˜„ | ìµœì‹  ë…¼ë¬¸ ì¬í˜„Â·ì„±ëŠ¥ ê°œì„ Â·í•™ìˆ  ë°œí‘œ |\n",
      "| **í”„ë¡œë•ì…˜í˜•** | ëª¨ë¸ì„ ì„œë¹„ìŠ¤ì— ì•ˆì •ì ìœ¼ë¡œ ì ìš© | íŒŒì´í”„ë¼ì¸ ìë™í™”Â·ì‹¤ì‹œê°„ ì¶”ë¡ Â·A/B í…ŒìŠ¤íŠ¸ |\n",
      "| **ë°ì´í„°í˜•** | ë°ì´í„° í’ˆì§ˆÂ·íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ | ë°ì´í„° ë¼ë²¨ë§Â·í”¼ì²˜ ìŠ¤í† ì–´ êµ¬ì¶•Â·ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ |\n",
      "| **MLOps ì—”ì§€ë‹ˆì–´** | AI ì‹œìŠ¤í…œ ìš´ì˜Â·CI/CD | ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬Â·ìë™ ì¬í•™ìŠµÂ·ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ |\n",
      "\n",
      "### ì‹œì‘í•˜ëŠ” ë°©ë²•\n",
      "\n",
      "1. **ê¸°ì´ˆ í•™ìŠµ**  \n",
      "   - Python, NumPy, pandas, scikitâ€‘learn â†’ ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ êµ¬í˜„  \n",
      "2. **ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬**  \n",
      "   - TensorFlow/Keras ë˜ëŠ” PyTorch íŠœí† ë¦¬ì–¼ ì§„í–‰  \n",
      "3. **í”„ë¡œì íŠ¸ ê²½í—˜**  \n",
      "   - Kaggle ëŒ€íšŒ, ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸, ê°œì¸ í¬íŠ¸í´ë¦¬ì˜¤(ì˜ˆ: ì´ë¯¸ì§€ ë¶„ë¥˜, í…ìŠ¤íŠ¸ ìš”ì•½)  \n",
      "4. **í´ë¼ìš°ë“œÂ·MLOps**  \n",
      "   - AWS SageMaker, GCP AI Platform, Azure ML ì¤‘ í•˜ë‚˜ ì„ íƒí•´ ëª¨ë¸ ë°°í¬ ì‹¤ìŠµ  \n",
      "5. **ìœ¤ë¦¬Â·ë²•ê·œ í•™ìŠµ**  \n",
      "   - AI ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸, GDPRÂ·CCPA ë“± ë°ì´í„° ë³´í˜¸ ê·œì • ì´í•´  \n",
      "\n",
      "---\n",
      "\n",
      "**ìš”ì•½**  \n",
      "AI ê°œë°œìëŠ” ë°ì´í„°ë¶€í„° ëª¨ë¸, ë°°í¬Â·ìš´ì˜, ê·¸ë¦¬ê³  ìœ¤ë¦¬Â·ë²•ê·œê¹Œì§€ AI ì‹œìŠ¤í…œ ì „ ê³¼ì •ì„ ì„¤ê³„Â·êµ¬í˜„Â·ê´€ë¦¬í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í”„ë¡œê·¸ë˜ë°Â·ìˆ˜í•™Â·ë¨¸ì‹ ëŸ¬ë‹ ì´ë¡ ì„ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ê³ , ìµœì‹  ëª¨ë¸ì„ í•™ìŠµÂ·ê²€ì¦í•œ ë’¤, í´ë¼ìš°ë“œÂ·ì»¨í…Œì´ë„ˆ í™˜ê²½ì— ì•ˆì •ì ìœ¼ë¡œ ì„œë¹„ìŠ¤í•©ë‹ˆë‹¤. ì§€ì†ì ì¸ í•™ìŠµê³¼ ì‹¤ì „ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ì„±ê³µì ì¸ AI ê°œë°œìì˜ í•µì‹¬ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# Groq APIë¥¼ ì‚¬ìš©í•˜ëŠ” ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    # invokeëŠ” LangChainì˜ Runnable ì¸í„°í˜ì´ìŠ¤ ë©”ì†Œë“œ\n",
    "    # invokeì˜ ë°˜í™˜ê°’ì€ ë³´í†µ AIMessage íƒ€ì…\n",
    "    print(\"ì‘ë‹µ: \", response.content)\n",
    "    # response.textëŠ” shortcut ì†ì„±ìœ¼ë¡œ ì œê³µë˜ê¸°ë„ í•˜ì§€ë§Œ, ê³µì‹ì ìœ¼ë¡  response.textë¥¼ ì“°ëŠ” ê²Œ ë‚«ë‹¤.\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜:: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1572707f",
   "metadata": {},
   "source": [
    "### ğŸ”‘ LCEL\n",
    "- LangChain Expression Language ì˜ ì•½ì\n",
    "- LangChainì˜ ë‹¤ì–‘í•œ ì»´í¬ë„ŒíŠ¸(í”„ë¡¬í”„íŠ¸, ëª¨ë¸, íŒŒì„œ, ë¦¬íŠ¸ë¦¬ë²„ ë“±)ë¥¼ í‘œì¤€í™”ëœ ë°©ì‹(Runnable ì¸í„°í˜ì´ìŠ¤) ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” ë¬¸ë²•\n",
    "- íŒŒì´í”„(|) ì—°ì‚°ìë¥¼ ì‚¬ìš©í•´ ë§ˆì¹˜ ìœ ë‹‰ìŠ¤ íŒŒì´í”„ë¼ì¸ì²˜ëŸ¼ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì—°ê²° ê°€ëŠ¥\n",
    "\n",
    "\n",
    "### âœ¨ LCELì˜ íŠ¹ì§•\n",
    "- ì„ ì–¸ì  êµ¬ì„±: \"ë¬´ì—‡ì„ í•˜ê³  ì‹¶ì€ì§€\"ë§Œ ì •ì˜ â†’ ë‚´ë¶€ ì‹¤í–‰ ë°©ì‹ì€ LangChainì´ ì²˜ë¦¬\n",
    "- ë™ê¸°/ë¹„ë™ê¸°/ìŠ¤íŠ¸ë¦¬ë° ì§€ì›: ê°™ì€ ì²´ì¸ì„ invoke, ainvoke, stream, astream ë“±ìœ¼ë¡œ í˜¸ì¶œ ê°€ëŠ¥\n",
    "- ë³‘ë ¬ ì‹¤í–‰ ìµœì í™”: ë…ë¦½ì ì¸ ë‹¨ê³„ëŠ” ìë™ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬\n",
    "- ì¬ì‹œë„/ëŒ€ì²´ ê²½ë¡œ: íŠ¹ì • ë‹¨ê³„ ì‹¤íŒ¨ ì‹œ fallback ì„¤ì • ê°€ëŠ¥\n",
    "- ì¤‘ê°„ ê²°ê³¼ ì ‘ê·¼: ì²´ì¸ ì‹¤í–‰ ì¤‘ê°„ ë‹¨ê³„ ê²°ê³¼ë„ í™•ì¸ ê°€ëŠ¥\n",
    "- ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ì…ì¶œë ¥ ê²€ì¦: Pydantic/JSONSchemaë¡œ ì…ë ¥Â·ì¶œë ¥ ìœ íš¨ì„± ê²€ì‚¬\n",
    "- LangSmith í†µí•©: ì‹¤í–‰ ë¡œê·¸ì™€ ë””ë²„ê¹… ì§€ì›\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f09cfcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangChain(ë­ì²´ì¸)**ì€ **ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)**ì„ í™œìš©í•œ **ì‘ìš© í”„ë¡œê·¸ë¨**ì„ **ì‰½ê²Œ ì„¤ê³„Â·êµ¬í˜„**í•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” **ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬**ì…ë‹ˆë‹¤.  \n",
      "\n",
      "---\n",
      "\n",
      "## 1. ì™œ í•„ìš”í• ê¹Œ?\n",
      "- LLM ìì²´ëŠ” â€œí…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì—”ì§„â€ì— ë¶ˆê³¼í•©ë‹ˆë‹¤.  \n",
      "- ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” **í”„ë¡¬í”„íŠ¸ ê´€ë¦¬**, **ë‹¤ì¤‘ ëª¨ë¸ ì—°ê³„**, **ì™¸ë¶€ ë°ì´í„°Â·ë„êµ¬ì™€ì˜ ì—°ë™**, **ëŒ€í™” ê¸°ì–µ(ë©”ëª¨ë¦¬)**, **ì—ëŸ¬ í•¸ë“¤ë§** ë“± ì—¬ëŸ¬ ë¶€ê°€ ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.  \n",
      "- LangChainì€ ì´ëŸ° ë¶€ê°€ ê¸°ëŠ¥ë“¤ì„ **ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸** í˜•íƒœë¡œ ì œê³µí•´, ê°œë°œìê°€ **í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§**ì— ì§‘ì¤‘í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. í•µì‹¬ ê°œë…\n",
      "\n",
      "| ê°œë… | ì„¤ëª… | ì˜ˆì‹œ |\n",
      "|------|------|------|\n",
      "| **PromptTemplate** | í”„ë¡¬í”„íŠ¸ë¥¼ ë³€ìˆ˜í™”Â·ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í…œí”Œë¦¿ìœ¼ë¡œ ì •ì˜ | `\"ë‹¤ìŒ ë¬¸ì¥ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”: {text}\"` |\n",
      "| **Chain** | ì—¬ëŸ¬ LLM í˜¸ì¶œÂ·í”„ë¡¬í”„íŠ¸Â·ë„êµ¬ë¥¼ **ìˆœì°¨** í˜¹ì€ **ë¶„ê¸°** í˜•íƒœë¡œ ì—°ê²° | `Prompt â†’ LLM â†’ Postâ€‘Processing` |\n",
      "| **Agent** | **ë„êµ¬(tool)** ë¥¼ ë™ì ìœ¼ë¡œ ì„ íƒÂ·í˜¸ì¶œí•˜ë©° ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” **ììœ¨ì ì¸** ì²´ê³„ | â€œë‚ ì”¨ë¥¼ ì•Œë ¤ì¤˜â€ â†’ ê²€ìƒ‰ ë„êµ¬ í˜¸ì¶œ â†’ LLM ìš”ì•½ |\n",
      "| **Memory** | ëŒ€í™”Â·ì‘ì—…ì˜ **ì»¨í…ìŠ¤íŠ¸**ë¥¼ ì €ì¥Â·ê´€ë¦¬í•´ ë‹¤ìŒ í˜¸ì¶œì— í™œìš© | ëŒ€í™” íˆìŠ¤í† ë¦¬, ì‘ì—… ì§„í–‰ ìƒí™© |\n",
      "| **Retriever** | ë²¡í„° DBÂ·ê²€ìƒ‰ ì—”ì§„ ë“±ì—ì„œ **ê´€ë ¨ ë¬¸ì„œ**ë¥¼ ì°¾ì•„ LLMì— ì œê³µ | RAG(Retrievalâ€‘Augmented Generation) |\n",
      "| **Tool** | ì™¸ë¶€ APIÂ·í•¨ìˆ˜Â·ìŠ¤í¬ë¦½íŠ¸ ë“± LLMì´ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” **ì‘ì—… ë‹¨ìœ„** | ê³„ì‚°ê¸°, ì›¹ ê²€ìƒ‰, ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ë“± |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. ì£¼ìš” êµ¬ì„± ìš”ì†Œì™€ íë¦„\n",
      "\n",
      "1. **PromptTemplate** â†’ 2. **LLM (OpenAI, Anthropic, Llama ë“±)** â†’ 3. **Chain**(í•„ìš” ì‹œ ì—¬ëŸ¬ ë‹¨ê³„ ì—°ê²°) â†’ 4. **Memory/Retriever**(ì»¨í…ìŠ¤íŠ¸ ë³´ê°•) â†’ 5. **Agent**(ë„êµ¬ ì„ íƒ) â†’ 6. **Output**  \n",
      "\n",
      "ì´ íë¦„ì„ íŒŒì´ì¬ ì½”ë“œë¡œ ê°„ë‹¨íˆ í‘œí˜„í•˜ë©´:\n",
      "\n",
      "```python\n",
      "from langchain import PromptTemplate, LLMChain, OpenAI\n",
      "\n",
      "# 1ï¸âƒ£ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
      "template = PromptTemplate(\n",
      "    input_variables=[\"question\"],\n",
      "    template=\"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆíˆ ë‹µë³€í•´ ì£¼ì„¸ìš”:\\n{question}\"\n",
      ")\n",
      "\n",
      "# 2ï¸âƒ£ LLM ì¸ìŠ¤í„´ìŠ¤ (ì˜ˆ: OpenAI GPTâ€‘3.5)\n",
      "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
      "\n",
      "# 3ï¸âƒ£ Chain ìƒì„±\n",
      "chain = LLMChain(prompt=template, llm=llm)\n",
      "\n",
      "# 4ï¸âƒ£ ì‹¤í–‰\n",
      "answer = chain.run({\"question\": \"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?\"})\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 4. ëŒ€í‘œì ì¸ ì‚¬ìš© ì‚¬ë¡€\n",
      "\n",
      "| ë¶„ì•¼ | êµ¬ì²´ì  ì˜ˆì‹œ |\n",
      "|------|------------|\n",
      "| **ì±—ë´‡** | ê³ ê° ë¬¸ì˜ ìë™ ì‘ë‹µ, FAQ ìƒì„±, ë©€í‹°í„´ ëŒ€í™” ë©”ëª¨ë¦¬ |\n",
      "| **ê²€ìƒ‰Â·ìš”ì•½** | RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ í›„ ìš”ì•½, ë…¼ë¬¸ ìš”ì•½ ì„œë¹„ìŠ¤ |\n",
      "| **ë°ì´í„° ë¶„ì„** | ìì—°ì–´ë¡œ ì§ˆë¬¸ â†’ SQL ìë™ ìƒì„± â†’ DB ì¡°íšŒ â†’ ê²°ê³¼ ë°˜í™˜ |\n",
      "| **ìë™í™”** | â€œì´ë²ˆ ì£¼ ë§¤ì¶œ ë³´ê³ ì„œ ë§Œë“¤ì–´ì¤˜â€ â†’ íŒŒì¼ ì½ê¸°Â·ë¶„ì„Â·ë³´ê³ ì„œ ì‘ì„± |\n",
      "| **ì½”ë“œ ìƒì„±Â·ë””ë²„ê¹…** | ìì—°ì–´ ëª…ì„¸ â†’ ì½”ë“œ ìŠ¤ë‹ˆí« ìƒì„± â†’ í…ŒìŠ¤íŠ¸Â·ìˆ˜ì • ë£¨í”„ |\n",
      "| **ë©€í‹°ëª¨ë‹¬** | í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ì…ë ¥ì„ ë°›ì•„ ì„¤ëª…Â·ë¶„ë¥˜Â·ìº¡ì…˜ ìƒì„± (LangChain + Vision ëª¨ë¸) |\n",
      "\n",
      "---\n",
      "\n",
      "## 5. ì¥ì  & ë‹¨ì \n",
      "\n",
      "### ì¥ì \n",
      "- **ëª¨ë“ˆí™”**: Prompt, LLM, Tool ë“±ì„ ë…ë¦½ì ìœ¼ë¡œ êµì²´Â·ì¡°í•© ê°€ëŠ¥  \n",
      "- **ìƒì‚°ì„±**: ë³µì¡í•œ RAG, Agent ë¡œì§ì„ ëª‡ ì¤„ ì½”ë“œë¡œ êµ¬í˜„  \n",
      "- **ë‹¤ì–‘í•œ ì—°ë™**: OpenAI, Azure, Cohere, HuggingFace, LangChain Hub ë“± í’ë¶€í•œ ëª¨ë¸Â·ë°ì´í„°ì†ŒìŠ¤ ì§€ì›  \n",
      "- **ì»¤ë®¤ë‹ˆí‹°**: í™œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ìƒíƒœê³„ì™€ ì˜ˆì œ, í…œí”Œë¦¿ì´ í’ë¶€  \n",
      "\n",
      "### ë‹¨ì \n",
      "- **ì¶”ìƒí™” ë¹„ìš©**: ì´ˆë³´ìì—ê²ŒëŠ” â€œChainâ€, â€œAgentâ€, â€œMemoryâ€ ê°œë…ì´ ë‹¤ì†Œ í—·ê°ˆë¦´ ìˆ˜ ìˆìŒ  \n",
      "- **ì„±ëŠ¥ ìµœì í™”**: ê¸°ë³¸ êµ¬í˜„ì´ í¸ë¦¬í•˜ì§€ë§Œ, ê³ ì„±ëŠ¥ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì§ì ‘ íŠœë‹ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ  \n",
      "- **ë²„ì „ ê´€ë¦¬**: ë¹ ë¥´ê²Œ ì—…ë°ì´íŠ¸ë˜ëŠ” íŒ¨í‚¤ì§€ë¼ API ë³€í™”ì— ì£¼ì˜ í•„ìš”  \n",
      "\n",
      "---\n",
      "\n",
      "## 6. ì‹œì‘í•˜ê¸° (ê°„ë‹¨ ê°€ì´ë“œ)\n",
      "\n",
      "1. **Python í™˜ê²½ ì¤€ë¹„**  \n",
      "   ```bash\n",
      "   pip install langchain openai  # ê¸°ë³¸ íŒ¨í‚¤ì§€\n",
      "   ```\n",
      "\n",
      "2. **OpenAI API í‚¤ ì„¤ì •**  \n",
      "   ```bash\n",
      "   export OPENAI_API_KEY=\"sk-...\"\n",
      "   ```\n",
      "\n",
      "3. **ì²« ë²ˆì§¸ ì²´ì¸ ë§Œë“¤ê¸°** (ìœ„ ì˜ˆì‹œ ì°¸ê³ )\n",
      "\n",
      "4. **LangChain Hub**ì—ì„œ ë¯¸ë¦¬ ë§Œë“  **Chain**Â·**Prompt**Â·**Agent** í…œí”Œë¦¿ì„ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥  \n",
      "   ```python\n",
      "   from langchain import hub\n",
      "   chain = hub.load(\"hwchase17/openai-chat\")\n",
      "   ```\n",
      "\n",
      "5. **ë°°í¬** â€“ FastAPI, Flask, LangServe ë“±ê³¼ ê²°í•©í•´ API í˜•íƒœë¡œ ì„œë¹„ìŠ¤ ê°€ëŠ¥  \n",
      "\n",
      "---\n",
      "\n",
      "## 7. ì°¸ê³  ìë£Œ\n",
      "\n",
      "- **ê³µì‹ ë¬¸ì„œ**: https://python.langchain.com/  \n",
      "- **GitHub**: https://github.com/langchain-ai/langchain  \n",
      "- **LangChain Hub**: ë‹¤ì–‘í•œ í”„ë¦¬ë¹ŒíŠ¸ ì²´ì¸Â·í”„ë¡¬í”„íŠ¸ ê³µìœ   \n",
      "- **íŠœí† ë¦¬ì–¼**: â€œLangChain Quickstartâ€ (YouTube, Medium)  \n",
      "\n",
      "---\n",
      "\n",
      "### í•œ ì¤„ ìš”ì•½\n",
      "> **LangChainì€ LLMì„ í™œìš©í•œ ë³µí•© ì• í”Œë¦¬ì¼€ì´ì…˜ì„ **í”„ë¡¬í”„íŠ¸, ì²´ì¸, ì—ì´ì „íŠ¸, ë©”ëª¨ë¦¬** ë“± ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±í•´ ë¹ ë¥´ê²Œ ë§Œë“¤ ìˆ˜ ìˆê²Œ í•´ ì£¼ëŠ” íŒŒì´ì¬ ê¸°ë°˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.**  \n",
      "\n",
      "ê¶ê¸ˆí•œ ì ì´ë‚˜ ì‹¤ì œ ì½”ë“œ êµ¬í˜„ì„ ë„ì™€ë“œë¦´ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸš€\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# StrOutputParserê°€ AIMessageë¥¼ ë°›ì•„ì„œ ìˆœìˆ˜ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì¤Œ.\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"prompt\":\"langchainì€ ë­ì•¼??\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3857e53",
   "metadata": {},
   "source": [
    "# ğŸ“ StrOutputParser ì •ë¦¬\n",
    "## 1. ê°œë…\n",
    "- LangChain Coreì—ì„œ ì œê³µí•˜ëŠ” OutputParser ì¤‘ í•˜ë‚˜\n",
    "- LLMì˜ ì‘ë‹µ(AIMessage)ì„ ìˆœìˆ˜ ë¬¸ìì—´(str) ë¡œ ë³€í™˜í•´ì£¼ëŠ” ì—­í• \n",
    "- ì¦‰, ëª¨ë¸ì´ ë°˜í™˜í•˜ëŠ” ë©”ì‹œì§€ ê°ì²´ì—ì„œ .content ë¶€ë¶„ë§Œ ë½‘ì•„ë‚´ì„œ ë¬¸ìì—´ë¡œ ëŒë ¤ì¤Œ\n",
    "\n",
    "## 2. ë™ì‘ ë°©ì‹\n",
    "- LLM â†’ AIMessage ê°ì²´ ìƒì„±\n",
    "(ì˜ˆ: AIMessage(content=\"ì•„ë©”ë¦¬ì¹´ë…¸ëŠ” 4,500ì›ì…ë‹ˆë‹¤.\"))\n",
    "- StrOutputParser â†’ AIMessage.content ë§Œ ì¶”ì¶œ\n",
    "- ìµœì¢… ë°˜í™˜ê°’: \"ì•„ë©”ë¦¬ì¹´ë…¸ëŠ” 4,500ì›ì…ë‹ˆë‹¤.\" (str íƒ€ì…)\n",
    "\n",
    "## 3. ì‚¬ìš© ì˜ˆì‹œ\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë„ˆëŠ” ì¹œì ˆí•œ ì¹´í˜ ì§ì›ì´ì•¼.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "### ëª¨ë¸\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "### íŒŒì„œ\n",
    "parser = StrOutputParser()\n",
    "\n",
    "### LCEL ì²´ì¸ êµ¬ì„±\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "### ì‹¤í–‰\n",
    "response = chain.invoke({\"question\": \"ì•„ë©”ë¦¬ì¹´ë…¸ ê°€ê²© ì•Œë ¤ì¤˜\"})\n",
    "print(type(response))   # <class 'str'>\n",
    "print(response)         # \"ì•„ë©”ë¦¬ì¹´ë…¸ëŠ” 4,500ì›ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "\n",
    "\n",
    "## 4. ì£¼ì˜í•  ì \n",
    "- StrOutputParserë¥¼ ì“°ë©´ ë°˜í™˜ê°’ì´ ë¬¸ìì—´ì´ë¯€ë¡œ response.content ë¥¼ ì“°ë©´ ì—ëŸ¬ ë°œìƒ\n",
    "- ê·¸ëƒ¥ response ìì²´ê°€ ìµœì¢… í…ìŠ¤íŠ¸ì„\n",
    "- ë§Œì•½ AIMessage ê°ì²´ ê·¸ëŒ€ë¡œ ì“°ê³  ì‹¶ë‹¤ë©´ StrOutputParser ë¥¼ ì²´ì¸ì— ë¶™ì´ì§€ ì•Šìœ¼ë©´ ë¨\n",
    "\n",
    "## 5. í™œìš© í¬ì¸íŠ¸\n",
    "- ë‹¨ìˆœíˆ í…ìŠ¤íŠ¸ ì‘ë‹µë§Œ í•„ìš”í•  ë•Œ ê°€ì¥ ê°„ë‹¨í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥\n",
    "- JSON, êµ¬ì¡°í™”ëœ ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤ë©´ JsonOutputParser, PydanticOutputParser ê°™ì€ ë‹¤ë¥¸ íŒŒì„œë¥¼ ì‚¬ìš©\n",
    "\n",
    "# ğŸ‘‰ ìš”ì•½:\n",
    "StrOutputParser = LLM ì‘ë‹µ(AIMessage) â†’ ë¬¸ìì—´(content)ë§Œ ì¶”ì¶œí•´ì£¼ëŠ” ë³€í™˜ê¸°\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee10d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-JHcERFOP-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
